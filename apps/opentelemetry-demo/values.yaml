# apps/opentelemetry-demo/values.yaml - FIXED ENV VARS

# 1. Disable all bundled observability tools (use our LGTM stack)
jaeger:
  enabled: false
prometheus:
  enabled: false
grafana:
  enabled: false
opensearch:
  enabled: false

# 2. Configure the demo's built-in OpenTelemetry Collector 
opentelemetry-collector:
  enabled: true
  
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch: {}
      memory_limiter:
        check_interval: 1s
        limit_mib: 256

    exporters:
      # Traces to Tempo via gRPC
      otlp/tempo:
        endpoint: "tempo.monitoring.svc.cluster.local:4317"
        tls:
          insecure: true
      
      # Metrics to Prometheus via remote write
      prometheusremotewrite:
        endpoint: "http://prometheus-stack-kube-prom-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
        
      # Logs to Loki via OTLP HTTP
      otlphttp:
        endpoint: "http://loki.monitoring.svc.cluster.local:3100/otlp"
        
      # Keep debug for troubleshooting
      debug:
        verbosity: detailed

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/tempo, debug]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite, debug]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlphttp, debug]

# 3. Override default configuration - Use envOverrides instead of env
default:
  envOverrides:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://{{ include \"otel-demo.name\" . }}-otelcol:4317"
    - name: OTEL_EXPORTER_OTLP_PROTOCOL
      value: "grpc"

# 4. Component-specific configuration
components:
  # Frontend proxy with ingress
  frontend-proxy:
    ingress:
      enabled: true
      ingressClassName: traefik
      annotations:
        external-dns.alpha.kubernetes.io/target: "e0995bb5-0641-44ba-9ac6-533e678117a2.cfargotunnel.com"
        external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
        traefik.ingress.kubernetes.io/router.middlewares: "monitoring-security-headers@kubernetescrd"
      hosts:
        - host: "otel-demo.milenika.dev"
          paths:
            - path: /
              pathType: Prefix
              port: 8080
      tls:
        - secretName: otel-demo-tls
          hosts:
            - "otel-demo.milenika.dev"

  # Load generator with reduced load for homelab
  load-generator:
    envOverrides:
      - name: LOCUST_USERS
        value: "2"
      - name: LOCUST_SPAWN_RATE
        value: "1"
      - name: LOCUST_HOST
        value: "http://frontend-proxy:8080"
    resources:
      limits:
        memory: "128Mi"
        cpu: "100m"
      requests:
        memory: "64Mi"
        cpu: "50m"

  # Resource limits for all services
  ad:
    resources:
      limits: { memory: "200Mi", cpu: "150m" }
      requests: { memory: "100Mi", cpu: "50m" }
      
  cart:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  checkout:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  currency:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  email:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  frontend:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  payment:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  product-catalog:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  recommendation:
    resources:
      limits: { memory: "300Mi", cpu: "200m" }
      requests: { memory: "150Mi", cpu: "100m" }
      
  shipping:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
