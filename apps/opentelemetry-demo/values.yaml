# apps/opentelemetry-demo/values.yaml - REMOVED GLOBAL ENV OVERRIDES

# 1. Disable all bundled observability tools (use our LGTM stack)
jaeger:
  enabled: false
prometheus:
  enabled: false
grafana:
  enabled: false
opensearch:
  enabled: false

# 2. Configure the demo's built-in OpenTelemetry Collector 
opentelemetry-collector:
  enabled: true
  
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        timeout: 1s
        send_batch_size: 512
      memory_limiter:
        check_interval: 1s
        limit_mib: 256

    exporters:
      # Traces to Tempo via gRPC
      otlp/tempo:
        endpoint: "tempo.monitoring.svc.cluster.local:4317"
        tls:
          insecure: true
      
      # Metrics to Prometheus via remote write
      prometheusremotewrite:
        endpoint: "http://prometheus-stack-kube-prom-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
        
      # Logs to Loki via OTLP HTTP
      otlphttp:
        endpoint: "http://loki.monitoring.svc.cluster.local:3100/otlp"
        
      # Keep debug for troubleshooting
      debug:
        verbosity: detailed

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/tempo, debug]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite, debug]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlphttp, debug]

# 3. Component-specific configuration
components:
  # Frontend proxy with ingress
  frontend-proxy:
    ingress:
      enabled: true
      ingressClassName: traefik
      annotations:
        external-dns.alpha.kubernetes.io/target: "e0995bb5-0641-44ba-9ac6-533e678117a2.cfargotunnel.com"
        external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
        traefik.ingress.kubernetes.io/router.middlewares: "monitoring-security-headers@kubernetescrd"
      hosts:
        - host: "otel-demo.milenika.dev"
          paths:
            - path: /
              pathType: Prefix
              port: 8080
      # tls:
      #   - secretName: otel-demo-tls
      #     hosts:
      #       - "otel-demo.milenika.dev"

  # Load generator with proper resource allocation (Python/Locust can be memory intensive)
  load-generator:
    envOverrides:
      - name: LOCUST_USERS
        value: "1"  # Increased from 2 for better demo traffic
      - name: LOCUST_SPAWN_RATE
        value: "1"
      - name: LOCUST_HOST
        value: "http://frontend-proxy:8080"
    resources:
      limits:
        memory: "2Gi"     # Load generators typically need 500MB-1GB
        cpu: "1000m"
      requests:
        memory: "512Mi"   # Start with 512MB 
        cpu: "250m"

  # Resource limits for all services
  ad:
    resources:
      limits: { memory: "200Mi", cpu: "150m" }
      requests: { memory: "100Mi", cpu: "50m" }
      
  cart:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  checkout:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  currency:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  email:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  frontend:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  payment:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  product-catalog:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
      
  recommendation:
    resources:
      limits: { memory: "300Mi", cpu: "200m" }
      requests: { memory: "150Mi", cpu: "100m" }
      
  shipping:
    resources:
      limits: { memory: "160Mi", cpu: "150m" }
      requests: { memory: "80Mi", cpu: "50m" }
