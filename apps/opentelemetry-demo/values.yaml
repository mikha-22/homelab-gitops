# apps/opentelemetry-demo/values.yaml

# Global settings
global:
  domain: milenika.dev

# Configure OpenTelemetry Collector to send data directly to your LGTM stack
opentelemetry-collector:
  enabled: true
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
        send_batch_max_size: 2048
      
      memory_limiter:
        limit_mib: 512
        spike_limit_mib: 128

      # ADDED: This processor groups spans belonging to the same trace together
      # This fixes the "<root span not yet received>" issue.
      groupbytrace:
        wait_duration: 3s # Wait up to 3 seconds for all spans in a trace
        num_traces: 1000

    connectors:
      spanmetrics:
        histogram:
          explicit:
            buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
        dimensions:
          - name: http.method
            default: GET
          - name: http.status_code
        exemplars:
          enabled: true
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
        metrics_flush_interval: 15s
        
    exporters:
      # Send traces to your Tempo instance
      otlp/tempo:
        endpoint: http://tempo.monitoring.svc.cluster.local:4317
        tls:
          insecure: true
      
      # Send metrics directly to your Prometheus
      prometheusremotewrite:
        endpoint: http://prometheus-stack-kube-prom-prometheus.monitoring.svc.cluster.local:9090/api/v1/write
        remote_write_queue:
          enabled: true
          queue_size: 10000
          num_consumers: 5
      
      # Send logs to your Loki
      loki:
        endpoint: http://loki-gateway.monitoring.svc.cluster.local:3100/loki/api/v1/push
        
      # Keep debug for troubleshooting
      debug:
        verbosity: normal
        
    service:
      telemetry:
        metrics:
          address: 0.0.0.0:8888
      pipelines:
        traces:
          receivers: [otlp]
          # UPDATED: Added 'groupbytrace' to the processing pipeline before batching.
          processors: [memory_limiter, groupbytrace, batch]
          exporters: [spanmetrics, otlp/tempo, debug]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite, debug]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [loki, debug]

# Frontend proxy with ingress for external access
components:
  frontend:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  frontendProxy:
    enabled: true
    resources:
      limits:
        memory: 100Mi
        cpu: 100m
      requests:
        memory: 50Mi
        cpu: 50m
    service:
      type: ClusterIP
    ingress:
      enabled: true
      ingressClassName: traefik
      annotations:
        external-dns.alpha.kubernetes.io/target: "e0995bb5-0641-44ba-9ac6-533e678117a2.cfargotunnel.com"
        external-dns.alpha.kubernetes.io/cloudflare-proxied": "true"
        traefik.ingress.kubernetes.io/router.middlewares": "monitoring-security-headers@kubernetescrd"
      hosts:
        - host: otel-demo.milenika.dev
          paths:
            - path: /
              pathType: Prefix
              port: 8080
      tls:
        - secretName: otel-demo-tls
          hosts:
            - otel-demo.milenika.dev

  # Other services - reduce resource usage for homelab
  adService:
    resources:
      limits:
        memory: 300Mi
        cpu: 200m
      requests:
        memory: 180Mi
        cpu: 100m

  cartService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 160Mi
        cpu: 100m

  checkoutService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  currencyService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  emailService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  paymentService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  productCatalogService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  recommendationService:
    resources:
      limits:
        memory: 500Mi # This one needs more memory
        cpu: 200m
      requests:
        memory: 200Mi
        cpu: 100m

  shippingService:
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m

  # Load generator - reduce load for homelab
  loadgenerator:
    enabled: true
    resources:
      limits:
        memory: 200Mi
        cpu: 200m
      requests:
        memory: 100Mi
        cpu: 100m
    env:
      - name: LOCUST_WEB_PORT
        value: "8089"
      - name: LOCUST_USERS
        value: "5" # Reduced load
      - name: LOCUST_SPAWN_RATE
        value: "1" # Slower spawn rate
      - name: LOCUST_HOST
        value: "http://frontend:8080"

# Disable components we don't need (you already have these)
jaeger:
  enabled: false

prometheus:
  enabled: false
  
grafana:
  enabled: false
  
opensearch:
  enabled: false
